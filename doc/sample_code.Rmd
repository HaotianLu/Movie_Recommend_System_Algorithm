---
title: "Project4"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

### Step 1 Load Data and Train-test Split
```{r download packages, eval = FALSE}
packages.used=c("dplyr", "tidyr","ggplot2","lsa")
# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

if(!require("krr")){
  install.packages("remotes")
  remotes::install_github("TimothyKBook/krr")
}
```

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(krr)
library(lsa)
```

```{r}
data <- read.csv("../data/ml-latest-small/ratings.csv")
set.seed(0)
## We want to make sure all movieId and all userId in the in the training dataset, we do a semi-random assignment

## randomly shuffle the row of the entire dataset
data <- data[sample(nrow(data)),]
## get a small dataset that contains all users and all movies
unique.user<-duplicated(data[,1])
unique.movie<-duplicated(data[,2])
index<-unique.user & unique.movie
all.user.movie <- data[!index,]

## split the remaining data into training and testing
rest <- data[index,]
test_idx <- sample(rownames(rest), round(nrow(data)/5, 0))
train_idx <- setdiff(rownames(rest), test_idx)

## combine the training with the previous dataset, which has all users and all movies
data_train <- rbind(all.user.movie, data[train_idx,])
data_test <- data[test_idx,]

## sort the training and testing data by userId then by movieId, 
## so when we update p and q, it is less likely to make mistakes
data_train <- arrange(data_train, userId, movieId)
data_test <- arrange(data_test, userId, movieId)
```

###Step 2 Matrix Factorization
#### Step 2.1 Algorithm (Alternating Least Squares) and  
#### Regularization (Penalty of Magnitudes + Bias and Intercepts)

```{r}
U <- length(unique(data$userId))
I <- length(unique(data$movieId))
source("../lib/ALS.R1R2.R")
```


#### Step 2.2 Parameter Tuning (lambda and f latent variables)

```{r}
source("../lib/ALS.Cross.Validation.R")
f_list <- c(10, 20)
l_list <- c(5, 10, 15)
f_l <- expand.grid(f_list, l_list)
```

```{r, eval=FALSE}
result_summary <- array(NA, dim = c(4, 10, nrow(f_l))) 
ALS.CV.Runtime <- system.time(for(i in 1:nrow(f_l)){
    par <- paste("f = ", f_l[i,1], ", lambda = ", f_l[i,2])
    cat(par, "\n")
    current_result <- als.cv(data, K = 5, f = f_l[i,1], lambda = f_l[i,2])
    result_summary[,,i] <- matrix(unlist(current_result), ncol = 10, byrow = T) 
    print(result_summary)
  
})
save(result_summary, file = "../output/alg.cv.rmse.Rdata")
```

```{r}
load("../output/alg.cv.rmse.Rdata")
rmse <- data.frame(rbind(t(result_summary[1,,]), t(result_summary[2,,])), train_test = rep(c("Train", "Test"), each = 6), par = rep(paste("f = ", f_l[,1], ", lambda = ", f_l[,2]), times = 2)) %>% gather("iteration", "RMSE", -train_test, -par)
rmse$iteration <- as.numeric(gsub("X", "", rmse$iteration))
rmse %>% ggplot(aes(x = iteration, y = RMSE, col = train_test)) + geom_point() + facet_wrap(~par, ncol=3)
```

### ALS result

```{r, eval= FALSE}
result <- ALS.R1R2(f = 10, lambda = 10, max.iter=3, data=data, train=data_train, test=data_test)
save(result, file = "../output/mat_fac.RData")
```

```{r load ALS result}
load(file = "../output/mat_fac.RData")
```

### Step 3 Postprocessing
#### SVD with KNN and 
#### SVD with Kernel Ridge Regression

```{r get similarity matrix, eval=FALSE}
similarity.matrix <- cosine(result$q)
colnames(similarity.matrix) <- levels(as.factor(data$movieId))
rownames(similarity.matrix) <- levels(as.factor(data$movieId))
save(similarity.matrix, file = "../output/similarity.matrix.Rdata")  
```

```{r KKN result}
load(file = "../output/similarity.matrix.Rdata")
source("../lib/KNN.Postprocessing.R")
KNN.result <- KNN.Post(data=data, train=data_train, test=data_test)
save(KNN.result, file = "../output/KNN.result.RData")
```

```{r ALG prediction + KNN prediction}
load("../output/KNN.result.RData")
source("../lib/Getting.pred.R")

data_train$als.pred <- apply(data_train, 1, get.pred, est_rating=result$ALS.rating)
data_train$mu <- rep(result$mu, nrow(data_train))
data_train$bi.pred <- result$bi[1,data_train$userId]
data_train$bu.pred <- result$bu[1,as.character(data_train$movieId)]
data_train$knn.pred <- apply(data_train, 1, get.pred, est_rating=KNN.result$knn.rating)

data_test$als.pred <- apply(data_test, 1, get.pred, est_rating=result$ALS.rating)
data_test$mu <- rep(result$mu, nrow(data_test))
data_test$bi.pred <- result$bi[1,data_test$userId]
data_test$bu.pred <- result$bu[1,as.character(data_test$movieId)]
data_test$knn.pred <- apply(data_test, 1, get.pred, est_rating=KNN.result$knn.rating)

## fit linear model
als.knn.model <- lm(rating ~ mu+bi.pred+bu.pred+knn.pred, data=data_train)

## get training prediction
als.knn.train <- predict(als.knn.model, data.frame(cbind(data_train$mu, data_train$bi.pred, data_train$bu.pred, data_train$knn.pred)))

## get testing prediction
als.krr.test <- predict(als.krr.model, data.frame(cbind(data_train$mu, data_test$bi.pred, data_test$bu.pred, data_test$knn.pred)))

## get training and testing RMSE
als.krr.train.rmse <- sqrt(mean((data_train$rating - als.knn.train)^2))
als.krr.test.rmse <- sqrt(mean((data_test$rating - als.knn.test)^2))

```


```{r KRR cv}
load(file-"../lib/KRR.Cross.Validation")

lambda=c(10,15,20)
krr.cv.rmse <- matrix(0, nrow = length(lambda), ncol = 4)
for(i in 1:length(lambda)){
    cat("lambda=", lambda[i], "\n")
    err_cv_svm[i,] <- krr.cv(dat_train=data, K.fold=5, lambda=lambda[i])
    save(krr.cv.rmse, file="../output/krr.cv.rmse.RData")
  }
```


```{r KRR cv visualization}
#Load visualization of cross validation results of svm
load("../output/krr.cv.rmse.RData")
krr.cv.rmse <- as.data.frame(krr.cv.rmse) 
colnames(krr.cv.rmse) <- c("mean_train_rmse", "mean_test_rmse", "sd_train_rmse", "sd_test_rmse")
lambda=c(10,15,20)
krr.cv.rmse$lambda = as.factor(lambda)
krr.cv.rmse %>% 
  ggplot(aes(x = lambda, y = mean_test_rmse,
             ymin = mean_test_rmse - sd_test_rmse, ymax = mean_test_rmse + sd_test_rmse)) + 
    geom_crossbar() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r KRR result}
KRR.result <- KRR.Post(lambda = 10, data=data, train = data_train, test=data_test)
save(KRR.result, file = "../output/KRR.result.RData")
```

```{r ALG prediction + KRR prediction}
load("../output/KRR.result.RData")

data_train$krr.pred <- apply(data_train, 1, get.pred, est_rating=KRR.result$krr.rating)
data_test$krr.pred <- apply(data_test, 1, get.pred, est_rating=KRR.result$krr.rating)

## fit linear model
als.krr.model <- lm(rating ~ als.pred+krr.pred, data=data_train)

## get training prediction
als.krr.train <- predict(als.krr.model, data.frame(cbind(als.pred=data_train$als.pred, krr.pred=data_train$krr.pred)))

## get testing prediction
als.krr.test <- predict(als.krr.model, data.frame(cbind(als.pred=data_test$als.pred, krr.pred=data_test$krr.pred)))

## get training and testing RMSE
als.krr.train.rmse <- sqrt(mean((data_train$rating - als.krr.train)^2))
als.krr.test.rmse <- sqrt(mean((data_test$rating - als.krr.test)^2))
```



### Step 4 Evaluation
You should visualize training and testing RMSE by different dimension of factors and epochs ([One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)). 

```{r}
library(ggplot2)

RMSE <- data.frame(epochs = seq(10, 100, 10), Training_MSE = result$train_RMSE, Test_MSE = result$test_RMSE) %>% gather(key = train_or_test, value = RMSE, -epochs)

RMSE %>% ggplot(aes(x = epochs, y = RMSE,col = train_or_test)) + geom_point() + scale_x_discrete(limits = seq(10, 100, 10)) + xlim(c(0, 100))

```
